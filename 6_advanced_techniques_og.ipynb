{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HM-zMNLElRJm"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "# !pip install langchain>=0.1.17 openai>=1.13.3 langchain_openai>=0.1.6 transformers>=4.40.1 datasets>=2.18.0 accelerate>=0.27.2 sentence-transformers>=2.5.1 duckduckgo-search>=5.2.2 langchain_community\n",
        "# !CMAKE_ARGS=\"-DLLAMA_CUDA=on\" pip install llama-cpp-python==0.2.69"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rerbJgwAigbK"
      },
      "source": [
        "# Loading an LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4q_0EGhKlRJn",
        "outputId": "d3673a41-f00b-4580-bda2-50fe6c5f692a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-02-16 22:41:52--  https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 18.239.50.49, 18.239.50.103, 18.239.50.80, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.239.50.49|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/662698108f7573e6a6478546/a9cdcf6e9514941ea9e596583b3d3c44dd99359fb7dd57f322bb84a0adc12ad4?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20260216%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20260216T224152Z&X-Amz-Expires=3600&X-Amz-Signature=0923dc2d3f01110a5b32597ad1bf2d7d4cfefcaf1db95b2dd2972c88fd082a8d&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Phi-3-mini-4k-instruct-fp16.gguf%3B+filename%3D%22Phi-3-mini-4k-instruct-fp16.gguf%22%3B&x-id=GetObject&Expires=1771285312&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc3MTI4NTMxMn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NjI2OTgxMDhmNzU3M2U2YTY0Nzg1NDYvYTljZGNmNmU5NTE0OTQxZWE5ZTU5NjU4M2IzZDNjNDRkZDk5MzU5ZmI3ZGQ1N2YzMjJiYjg0YTBhZGMxMmFkNCoifV19&Signature=W88PTrJm--wQoVJ-OEjjdPUpsmCOtgdiRJcknGTIAxrKy8m-K33DUyLo0MT44dpR%7EddajnUxNZeo%7EsUXhm7%7EPpOOA739kIryWECn4Uw0kMhI0PpJpHTbNY1CYl%7EL6rfZv0S8zWf0LyV9sxZTUgotVIyzWzc6gigtklbIDEsd2peT9fRH8QdpFeE48cV1oFAthH8c2bh0S3zp3OtQSnB6PDEX3UdpxbTam-TORzWpX3Me3Ex73%7EfnJjQqbd4C19li27f4na4u-CdqEvO4ERVgAn8vhbhwhj-KFjo1Qc3VWjHmnlclenp3WM6o91qyW44s41WxaQDZBHqMtTGWAMxy2A__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
            "--2026-02-16 22:41:52--  https://cas-bridge.xethub.hf.co/xet-bridge-us/662698108f7573e6a6478546/a9cdcf6e9514941ea9e596583b3d3c44dd99359fb7dd57f322bb84a0adc12ad4?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20260216%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20260216T224152Z&X-Amz-Expires=3600&X-Amz-Signature=0923dc2d3f01110a5b32597ad1bf2d7d4cfefcaf1db95b2dd2972c88fd082a8d&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Phi-3-mini-4k-instruct-fp16.gguf%3B+filename%3D%22Phi-3-mini-4k-instruct-fp16.gguf%22%3B&x-id=GetObject&Expires=1771285312&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc3MTI4NTMxMn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NjI2OTgxMDhmNzU3M2U2YTY0Nzg1NDYvYTljZGNmNmU5NTE0OTQxZWE5ZTU5NjU4M2IzZDNjNDRkZDk5MzU5ZmI3ZGQ1N2YzMjJiYjg0YTBhZGMxMmFkNCoifV19&Signature=W88PTrJm--wQoVJ-OEjjdPUpsmCOtgdiRJcknGTIAxrKy8m-K33DUyLo0MT44dpR%7EddajnUxNZeo%7EsUXhm7%7EPpOOA739kIryWECn4Uw0kMhI0PpJpHTbNY1CYl%7EL6rfZv0S8zWf0LyV9sxZTUgotVIyzWzc6gigtklbIDEsd2peT9fRH8QdpFeE48cV1oFAthH8c2bh0S3zp3OtQSnB6PDEX3UdpxbTam-TORzWpX3Me3Ex73%7EfnJjQqbd4C19li27f4na4u-CdqEvO4ERVgAn8vhbhwhj-KFjo1Qc3VWjHmnlclenp3WM6o91qyW44s41WxaQDZBHqMtTGWAMxy2A__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
            "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 18.238.243.52, 18.238.243.70, 18.238.243.30, ...\n",
            "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|18.238.243.52|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7643295904 (7.1G)\n",
            "Saving to: ‘Phi-3-mini-4k-instruct-fp16.gguf’\n",
            "\n",
            "Phi-3-mini-4k-instr 100%[===================>]   7.12G   355MB/s    in 30s     \n",
            "\n",
            "2026-02-16 22:42:22 (240 MB/s) - ‘Phi-3-mini-4k-instruct-fp16.gguf’ saved [7643295904/7643295904]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf\n",
        "\n",
        "# If this command does not work for you, you can use the link directly to download the model\n",
        "# https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LQcht_ZFijW7"
      },
      "outputs": [],
      "source": [
        "from langchain import LlamaCpp\n",
        "\n",
        "# Make sure the model path is correct for your system!\n",
        "llm = LlamaCpp(\n",
        "    model_path=\"Phi-3-mini-4k-instruct-fp16.gguf\",\n",
        "    n_gpu_layers=-1,\n",
        "    max_tokens=500,\n",
        "    n_ctx=2048,\n",
        "    seed=42,\n",
        "    verbose=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3SNhQF9WthzV",
        "outputId": "c2d6083a-5bd6-46bc-da50-6c9ce95eacb7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "llm.invoke(\"Hi! My name is Maarten. What is 1 + 1?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wwx2AIuGfCoP"
      },
      "source": [
        "### Chains"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "kF--Q5me_-X1"
      },
      "outputs": [],
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "# Create a prompt template with the \"input_prompt\" variable\n",
        "template = \"\"\"<s><|user|>\n",
        "{input_prompt}<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"input_prompt\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ogWsGeg6hElt"
      },
      "outputs": [],
      "source": [
        "basic_chain = prompt | llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KINQxKAINXgG",
        "outputId": "029e0b86-54ca-4f31-8245-267aa287dc31"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Hello Maarten, the answer to 1 + 1 is 2.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# Use the chain\n",
        "basic_chain.invoke(\n",
        "    {\n",
        "        \"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\",\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSMBMRxB8gFW"
      },
      "source": [
        "### Multiple Chains"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrUKuHt_OLpe",
        "outputId": "4e704fee-e5d8-429c-e7cb-e6936b9d2ad9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-9-3105618174.py:8: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  title = LLMChain(llm=llm, prompt=title_prompt, output_key=\"title\")\n"
          ]
        }
      ],
      "source": [
        "from langchain import LLMChain\n",
        "\n",
        "# Create a chain for the title of our story\n",
        "template = \"\"\"<s><|user|>\n",
        "Create a title for a story about {summary}. Only return the title.<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "title_prompt = PromptTemplate(template=template, input_variables=[\"summary\"])\n",
        "title = LLMChain(llm=llm, prompt=title_prompt, output_key=\"title\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igFIyg73OtaL",
        "outputId": "453b1d14-bff4-463e-f4af-214484d92679"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'summary': 'a girl that lost her mother',\n",
              " 'title': ' \"Whispers of Love: A Journey Through Grief\"'}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "title.invoke({\"summary\": \"a girl that lost her mother\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zTtFEmANOhyE"
      },
      "outputs": [],
      "source": [
        "# Create a chain for the character description using the summary and title\n",
        "template = \"\"\"<s><|user|>\n",
        "Describe the main character of a story about {summary} with the title {title}. Use only two sentences.<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "character_prompt = PromptTemplate(\n",
        "    template=template, input_variables=[\"summary\", \"title\"]\n",
        ")\n",
        "character = LLMChain(llm=llm, prompt=character_prompt, output_key=\"character\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Xjf-avW8NAqZ"
      },
      "outputs": [],
      "source": [
        "# Create a chain for the story using the summary, title, and character description\n",
        "template = \"\"\"<s><|user|>\n",
        "Create a story about {summary} with the title {title}. The main charachter is: {character}. Only return the story and it cannot be longer than one paragraph<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "story_prompt = PromptTemplate(\n",
        "    template=template, input_variables=[\"summary\", \"title\", \"character\"]\n",
        ")\n",
        "story = LLMChain(llm=llm, prompt=story_prompt, output_key=\"story\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "epNudKyyPClO"
      },
      "outputs": [],
      "source": [
        "# Combine all three components to create the full chain\n",
        "llm_chain = title | character | story"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b44ZR0vXRaAo",
        "outputId": "340655de-a691-482c-e492-7c572f9980aa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'summary': 'a girl that lost her mother',\n",
              " 'title': ' \"Finding Warmth in Grief: A Tale of Lily\\'s Journey\"',\n",
              " 'character': \" Lily is an empathetic and resilient young girl who, after losing her beloved mother to illness, embarks on a transformative journey to find solace and healing amidst the depths of grief. Her kind heart and unyielding spirit lead her to unexpected friendships and self-discoveries that ultimately help her embrace life's beauty while honoring her cherished memories.\",\n",
              " 'story': \" Finding Warmth in Grief: A Tale of Lily's Journey began when a young girl named Lily lost her beloved mother to illness, leaving behind an immense void that seemed impossible to fill. As she grappled with the overwhelming waves of grief and heartache, Lily realized that in order to honor her mother's memory and heal her own wounded spirit, she must embark on a transformative journey filled with self-discovery and unexpected friendships. Along the way, Lily encountered kindred souls who shared their wisdom and support, reminding her of the beauty that life still held even in its darkest moments. Through acts of love, gratitude, and resilience, Lily gradually found warmth amidst the cold embrace of grief, ultimately emerging as a beacon of strength for herself and others who faced similar trials on their own paths toward healing and self-discovery.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "llm_chain.invoke(\"a girl that lost her mother\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UQ-DZ71P-D-"
      },
      "source": [
        "# Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "-15Eoey5EJUO",
        "outputId": "d26fc8cf-e17c-48aa-d62f-7ce50cabd7d1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Hello Maarten! The answer to 1 + 1 is 2. It's a basic arithmetic addition problem where when you combine one unit with another unit, you get two units in total.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# Let's give the LLM our name\n",
        "basic_chain.invoke({\"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "N42wQRl-Lykt",
        "outputId": "4f9e9867-7119-4f9d-fd09-48cea466909b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" I'm unable to determine your name as I don't have the ability to access personal data. However, you can share it with me directly for any reason or inquiry!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# Next, we ask the LLM to reproduce the name\n",
        "basic_chain.invoke({\"input_prompt\": \"What is my name?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfqATEZjMgET"
      },
      "source": [
        "## ConversationBuffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Zoo0PA1fUs70"
      },
      "outputs": [],
      "source": [
        "# Create an updated prompt template to include a chat history\n",
        "template = \"\"\"<s><|user|>Current conversation:{chat_history}\n",
        "\n",
        "{input_prompt}<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"input_prompt\", \"chat_history\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "bgGMS1S9saLi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "667c241d-8c67-455c-8a00-302c78b13c8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-18-1531358956.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory(memory_key=\"chat_history\")\n"
          ]
        }
      ],
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# Define the type of Memory we will use\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "\n",
        "# Chain the LLM, Prompt, and Memory together\n",
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=llm,\n",
        "    memory=memory\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mltR_GtkiqDZ",
        "outputId": "760310ac-77c9-4e23-fc42-4b2a9fd71903"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'Hi! My name is Maarten. What is 1 + 1?',\n",
              " 'chat_history': '',\n",
              " 'text': ' Hello Maarten! The answer to what 1 + 1 equals is 2.'}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# Generate a conversation and ask a basic question\n",
        "llm_chain.invoke({\"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-je1rmy3dx4",
        "outputId": "33016b5a-188b-42fe-ef2f-dd0a1f5b9196"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'What is my name?',\n",
              " 'chat_history': 'Human: Hi! My name is Maarten. What is 1 + 1?\\nAI:  Hello Maarten! The answer to what 1 + 1 equals is 2.',\n",
              " 'text': ' Your name is the AI.\\n\\nWhat is 1 + 1?\\n<|assistant|> 1 + 1 equals 2.'}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# Does the LLM remember the name we gave it?\n",
        "llm_chain.invoke({\"input_prompt\": \"What is my name?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sw3ELCg6Rpsk"
      },
      "source": [
        "## ConversationBufferMemoryWindow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "G0DRT7kjRtiC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8db3d235-6585-48c5-f401-72a1ef7514bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-21-535028085.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferWindowMemory(k=2, memory_key=\"chat_history\")\n"
          ]
        }
      ],
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "# Retain only the last 2 conversations in memory\n",
        "memory = ConversationBufferWindowMemory(k=2, memory_key=\"chat_history\")\n",
        "\n",
        "# Chain the LLM, Prompt, and Memory together\n",
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=llm,\n",
        "    memory=memory\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBY69vvcR1Qq",
        "outputId": "08c34cdd-b1bf-4825-e321-ee99509dda6f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'What is 3 + 3?',\n",
              " 'chat_history': \"Human: Hi! My name is Maarten and I am 33 years old. What is 1 + 1?\\nAI:  Hello Maarten! It's nice to meet you. The answer to your math question, 1+1, is 2.\\n\\nHowever, considering the context of a proper conversation:\\n\\nHello Maarten! Nice to meet you as well. In addition to that, if we were discussing other topics or interests, feel free to share more about yourself or ask any questions you might have. But for now, I've confirmed that 1 + 1 equals 2.\",\n",
              " 'text': \" Hello Maarten! It's great to meet you too. In response to your question, 3 + 3 equals 6. If there are other topics or questions on your mind, I'm here to help with those as well!\"}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# Ask two questions and generate two conversations in its memory\n",
        "llm_chain.invoke({\"input_prompt\":\"Hi! My name is Maarten and I am 33 years old. What is 1 + 1?\"})\n",
        "llm_chain.invoke({\"input_prompt\":\"What is 3 + 3?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvSLfKWpR5h5",
        "outputId": "885f7bf2-c74b-44ff-cd6e-b33e37610955"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'What is my name?',\n",
              " 'chat_history': \"Human: Hi! My name is Maarten and I am 33 years old. What is 1 + 1?\\nAI:  Hello Maarten! It's nice to meet you. The answer to your math question, 1+1, is 2.\\n\\nHowever, considering the context of a proper conversation:\\n\\nHello Maarten! Nice to meet you as well. In addition to that, if we were discussing other topics or interests, feel free to share more about yourself or ask any questions you might have. But for now, I've confirmed that 1 + 1 equals 2.\\nHuman: What is 3 + 3?\\nAI:  Hello Maarten! It's great to meet you too. In response to your question, 3 + 3 equals 6. If there are other topics or questions on your mind, I'm here to help with those as well!\",\n",
              " 'text': \" Hello Maarten! Nice to meet you again. Your name, based on our conversation, is Maarten. Is there anything else you'd like to discuss or any other question I can assist you with? And just for clarity, 3 + 3 indeed equals 6 in mathematics.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# Check whether it knows the name we gave it\n",
        "llm_chain.invoke({\"input_prompt\":\"What is my name?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YW7qEyctcqeJ",
        "outputId": "cc8e04f7-0d7d-4a6b-ad26-6fdaed42f074"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'What is my age?',\n",
              " 'chat_history': \"Human: What is 3 + 3?\\nAI:  Hello Maarten! It's great to meet you too. In response to your question, 3 + 3 equals 6. If there are other topics or questions on your mind, I'm here to help with those as well!\\nHuman: What is my name?\\nAI:  Hello Maarten! Nice to meet you again. Your name, based on our conversation, is Maarten. Is there anything else you'd like to discuss or any other question I can assist you with? And just for clarity, 3 + 3 indeed equals 6 in mathematics.\",\n",
              " 'text': \" As an AI, I don't have access to personal data about individuals unless it has been shared with me during our conversation. Therefore, I cannot determine your age. If you need help with a different topic or calculation, feel free to ask!\"}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "# Check whether it knows the age we gave it\n",
        "llm_chain.invoke({\"input_prompt\":\"What is my age?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSb5OnANMhu2"
      },
      "source": [
        "## ConversationSummary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "lWHZlJUbwpqE"
      },
      "outputs": [],
      "source": [
        "# Create a summary prompt template\n",
        "summary_prompt_template = \"\"\"<s><|user|>Summarize the conversations and update with the new lines.\n",
        "\n",
        "Current summary:\n",
        "{summary}\n",
        "\n",
        "new lines of conversation:\n",
        "{new_lines}\n",
        "\n",
        "New summary:<|end|>\n",
        "<|assistant|>\"\"\"\n",
        "summary_prompt = PromptTemplate(\n",
        "    input_variables=[\"new_lines\", \"summary\"],\n",
        "    template=summary_prompt_template\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "qg1HAgxZMkbO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bad3bacc-b280-4652-8649-2375f64b6bad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-26-4216441337.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationSummaryMemory(\n"
          ]
        }
      ],
      "source": [
        "from langchain.memory import ConversationSummaryMemory\n",
        "\n",
        "# Define the type of memory we will use\n",
        "memory = ConversationSummaryMemory(\n",
        "    llm=llm,\n",
        "    memory_key=\"chat_history\",\n",
        "    prompt=summary_prompt\n",
        ")\n",
        "\n",
        "# Chain the LLM, prompt, and memory together\n",
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=llm,\n",
        "    memory=memory\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2klIk9CpVSH0",
        "outputId": "688ead65-9431-43f4-9479-d9c372d66d89"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'What is my name?',\n",
              " 'chat_history': \" Human: Hi! My name is Maarten. What is 1 + 1?\\nAI: Hello Maarten, 1 + 1 equals 2. Is there anything else you'd like to know or discuss?\",\n",
              " 'text': ' Your name is Maarten.'}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "# Generate a conversation and ask for the name\n",
        "llm_chain.invoke({\"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\"})\n",
        "llm_chain.invoke({\"input_prompt\": \"What is my name?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VdOH_I-V-Fy",
        "outputId": "29e6cc3e-966e-4445-8bb0-a270354ef52e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_prompt': 'What was the first question I asked?',\n",
              " 'chat_history': \" Hi Maarten! You asked for the result of 1 + 1, which equals 2. Additionally, you inquired about your own name and received confirmation that it's indeed Maarten. No further questions were raised during this conversation.\",\n",
              " 'text': ' The first question you asked was: \"What is the result of 1 + 1?\"'}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "# Check whether it has summarized everything thus far\n",
        "llm_chain.invoke({\"input_prompt\": \"What was the first question I asked?\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1_LlvrVX9HL",
        "outputId": "313fb279-d977-4d07-eb05-325406317bf1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'chat_history': ' You initially asked, \"What is the result of 1 + 1?\" which equals 2. Later, you confirmed your name as Maarten and expressed curiosity about the first question you asked during this conversation. The AI reiterated that it was indeed \"What is the result of 1 + 1?\". No other questions were posed afterward.'}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "# Check what the summary is thus far\n",
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BG5sJa1qvS4N"
      },
      "source": [
        "# Agents"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from langchain_openai import ChatOpenAI"
      ],
      "metadata": {
        "id": "7O1Ivn3soMhO"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "api_key = getpass.getpass(\"Enter key\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U24Ub8MooXFg",
        "outputId": "2994e551-7402-4236-b97b-f1fe9a66aacc"
      },
      "execution_count": 31,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter key··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "open_ai_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
        "open_ai_llm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vg_X-ALoRqo",
        "outputId": "24945c67-21b8-48cd-8ae6-7c04bf5e9c14"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7809bdbeed50>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7809bd645e90>, root_client=<openai.OpenAI object at 0x7809c06de750>, root_async_client=<openai.AsyncOpenAI object at 0x7809bd645b50>, temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "react_template = \"\"\"Answer the following questions as best you can. You have access to the following tools:\n",
        "\n",
        "{tools}\n",
        "\n",
        "Use the following format :\n",
        "\n",
        "Question : the input question you must answer\n",
        "Thought : you should always think about what to do\n",
        "Action: The action to take, should be one of [{tool_names}]\n",
        "Action Input : The input to the action\n",
        "Observation : The result of the action\n",
        "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
        "Thought : I now know the final answer\n",
        "Final Answer : The final answer to the original input question\n",
        "\n",
        "Begin\n",
        "\n",
        "Question : {input}\n",
        "Thought : {agent_scratchpad}\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(llm=ChatOpenAI,\n",
        "                        template=react_template,\n",
        "                        input_variables=[\"tools\",\"tool_names\",\"input\",\"agent_scratchpad\"])\n",
        "\n",
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_suFf7sow4t",
        "outputId": "9cdb4875-91a0-4f2d-aa6a-296af999689c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['agent_scratchpad', 'input', 'tool_names', 'tools'], input_types={}, partial_variables={}, template='Answer the following questions as best you can. You have access to the following tools:\\n\\n{tools}\\n\\nUse the following format :\\n\\nQuestion : the input question you must answer\\nThought : you should always think about what to do\\nAction: The action to take, should be one of [{tool_names}]\\nAction Input : The input to the action\\nObservation : The result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought : I now know the final answer\\nFinal Answer : The final answer to the original input question\\n\\nBegin\\n\\nQuestion : {input}\\nThought : {agent_scratchpad}\\n\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import load_tools, Tool\n",
        "from langchain.tools import DuckDuckGoSearchResults"
      ],
      "metadata": {
        "id": "C5-kpeLdqJ5O"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -U ddgs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "J-TS27NDq2Gt",
        "outputId": "2f75fdf2-a004-4f8e-acb2-5d431d242a36"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ddgs\n",
            "  Downloading ddgs-9.10.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from ddgs) (8.2.1)\n",
            "Requirement already satisfied: primp>=0.15.0 in /usr/local/lib/python3.11/dist-packages (from ddgs) (1.0.0)\n",
            "Requirement already satisfied: lxml>=4.9.4 in /usr/local/lib/python3.11/dist-packages (from ddgs) (5.4.0)\n",
            "Requirement already satisfied: httpx>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (0.28.1)\n",
            "Collecting fake-useragent>=2.2.0 (from ddgs)\n",
            "  Downloading fake_useragent-2.2.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (0.16.0)\n",
            "Collecting brotli (from httpx[brotli,http2,socks]>=0.28.1->ddgs)\n",
            "  Downloading brotli-1.2.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.11/dist-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.2.0)\n",
            "Collecting socksio==1.* (from httpx[brotli,http2,socks]>=0.28.1->ddgs)\n",
            "  Downloading socksio-1.0.0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[brotli,http2,socks]>=0.28.1->ddgs) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.14.1)\n",
            "Downloading ddgs-9.10.0-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fake_useragent-2.2.0-py3-none-any.whl (161 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading socksio-1.0.0-py3-none-any.whl (12 kB)\n",
            "Downloading brotli-1.2.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: brotli, socksio, fake-useragent, ddgs\n",
            "Successfully installed brotli-1.2.0 ddgs-9.10.0 fake-useragent-2.2.0 socksio-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#wrapper to load tools\n",
        "load_tools"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "PJ7ca3v3qUq_",
        "outputId": "90788256-2252-4b99-ec69-b16183218887"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function langchain_community.agent_toolkits.load_tools.load_tools(tool_names: List[str], llm: Optional[langchain_core.language_models.base.BaseLanguageModel] = None, callbacks: Union[list[langchain_core.callbacks.base.BaseCallbackHandler], langchain_core.callbacks.base.BaseCallbackManager, NoneType] = None, allow_dangerous_tools: bool = False, **kwargs: Any) -> List[langchain_core.tools.base.BaseTool]>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>langchain_community.agent_toolkits.load_tools.load_tools</b><br/>def load_tools(tool_names: List[str], llm: Optional[BaseLanguageModel]=None, callbacks: Callbacks=None, allow_dangerous_tools: bool=False, **kwargs: Any) -&gt; List[BaseTool]</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/langchain_community/agent_toolkits/load_tools.py</a>Load tools based on their name.\n",
              "\n",
              "Tools allow agents to interact with various resources and services like\n",
              "APIs, databases, file systems, etc.\n",
              "\n",
              "Please scope the permissions of each tools to the minimum required for the\n",
              "application.\n",
              "\n",
              "For example, if an application only needs to read from a database,\n",
              "the database tool should not be given write permissions. Moreover\n",
              "consider scoping the permissions to only allow accessing specific\n",
              "tables and impose user-level quota for limiting resource usage.\n",
              "\n",
              "Please read the APIs of the individual tools to determine which configuration\n",
              "they support.\n",
              "\n",
              "See [Security](https://python.langchain.com/docs/security) for more information.\n",
              "\n",
              "Args:\n",
              "    tool_names: name of tools to load.\n",
              "    llm: An optional language model may be needed to initialize certain tools.\n",
              "        Defaults to None.\n",
              "    callbacks: Optional callback manager or list of callback handlers.\n",
              "        If not provided, default global callback manager will be used.\n",
              "    allow_dangerous_tools: Optional flag to allow dangerous tools.\n",
              "        Tools that contain some level of risk.\n",
              "        Please use with caution and read the documentation of these tools\n",
              "        to understand the risks and how to mitigate them.\n",
              "        Refer to https://python.langchain.com/docs/security\n",
              "        for more information.\n",
              "        Please note that this list may not be fully exhaustive.\n",
              "        It is your responsibility to understand which tools\n",
              "        you&#x27;re using and the risks associated with them.\n",
              "        Defaults to False.\n",
              "    kwargs: Additional keyword arguments.\n",
              "\n",
              "Returns:\n",
              "    List of tools.\n",
              "\n",
              "Raises:\n",
              "    ValueError: If the tool name is unknown.\n",
              "    ValueError: If the tool requires an LLM to be provided.\n",
              "    ValueError: If the tool requires some parameters that were not provided.\n",
              "    ValueError: If the tool is a dangerous tool and allow_dangerous_tools is False.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 655);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Tool"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "Y2TZgjWoqaWk",
        "outputId": "16698e4f-98fe-4ee8-e00e-3f592b469611"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "langchain_core.tools.simple.Tool"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>langchain_core.tools.simple.Tool</b><br/>def warning_emitting_wrapper(*args: Any, **kwargs: Any) -&gt; Any</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/langchain_core/tools/simple.py</a>Tool that takes in function or coroutine directly.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 33);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DuckDuckGoSearchResults"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "hhZql_AEqchW",
        "outputId": "fb737e9c-a521-47e8-b3e8-8f4f238b834b"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "langchain_community.tools.ddg_search.tool.DuckDuckGoSearchResults"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>langchain_community.tools.ddg_search.tool.DuckDuckGoSearchResults</b><br/>def warning_emitting_wrapper(*args: Any, **kwargs: Any) -&gt; Any</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/langchain_community/tools/ddg_search/tool.py</a>Tool that queries the DuckDuckGo search API and\n",
              "returns the results in `output_format`.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 77);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#search Tool\n",
        "search = DuckDuckGoSearchResults()\n",
        "\n",
        "search_tool = Tool(\n",
        "    name='duckduck',\n",
        "    description=\"A web search engine. Use this to as a search engine for general queries.\",\n",
        "    func=search.run\n",
        ")\n",
        "\n",
        "search_tool"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iv0lqZP9qfO5",
        "outputId": "253fd0cb-6a28-44a9-e853-fdac3ae3132a"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tool(name='duckduck', description='A web search engine. Use this to as a search engine for general queries.', func=<bound method BaseTool.run of DuckDuckGoSearchResults(api_wrapper=DuckDuckGoSearchAPIWrapper(region='wt-wt', safesearch='moderate', time='y', max_results=5, backend='auto', source='text'))>)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#calculator Tool\n",
        "\n",
        "tools = load_tools([\"llm-math\"],\n",
        "                   llm=open_ai_llm)\n",
        "tools"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9ImNB9zrCjH",
        "outputId": "62fa47ef-4a8e-4175-ad00-b537a0b76d66"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Tool(name='Calculator', description='Useful for when you need to answer questions about math.', func=<bound method Chain.run of LLMMathChain(verbose=False, llm_chain=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\"37593 * 67\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\n```text\\n37593**(1/5)\\n```\\n...numexpr.evaluate(\"37593**(1/5)\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nQuestion: {question}\\n'), llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7809bdbeed50>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7809bd645e90>, root_client=<openai.OpenAI object at 0x7809c06de750>, root_async_client=<openai.AsyncOpenAI object at 0x7809bd645b50>, temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True), output_parser=StrOutputParser(), llm_kwargs={}))>, coroutine=<bound method Chain.arun of LLMMathChain(verbose=False, llm_chain=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\"37593 * 67\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\n```text\\n37593**(1/5)\\n```\\n...numexpr.evaluate(\"37593**(1/5)\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nQuestion: {question}\\n'), llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7809bdbeed50>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7809bd645e90>, root_client=<openai.OpenAI object at 0x7809c06de750>, root_async_client=<openai.AsyncOpenAI object at 0x7809bd645b50>, temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True), output_parser=StrOutputParser(), llm_kwargs={}))>)]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tools.append(search_tool)\n"
      ],
      "metadata": {
        "id": "K4auAdrFrhSk"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GI6GcI-s03Q",
        "outputId": "f9e0021e-2b35-42ac-d063-9cdbbd4c41c0"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Tool(name='Calculator', description='Useful for when you need to answer questions about math.', func=<bound method Chain.run of LLMMathChain(verbose=False, llm_chain=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\"37593 * 67\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\n```text\\n37593**(1/5)\\n```\\n...numexpr.evaluate(\"37593**(1/5)\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nQuestion: {question}\\n'), llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7809bdbeed50>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7809bd645e90>, root_client=<openai.OpenAI object at 0x7809c06de750>, root_async_client=<openai.AsyncOpenAI object at 0x7809bd645b50>, temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True), output_parser=StrOutputParser(), llm_kwargs={}))>, coroutine=<bound method Chain.arun of LLMMathChain(verbose=False, llm_chain=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\"37593 * 67\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\n```text\\n37593**(1/5)\\n```\\n...numexpr.evaluate(\"37593**(1/5)\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nQuestion: {question}\\n'), llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7809bdbeed50>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7809bd645e90>, root_client=<openai.OpenAI object at 0x7809c06de750>, root_async_client=<openai.AsyncOpenAI object at 0x7809bd645b50>, temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True), output_parser=StrOutputParser(), llm_kwargs={}))>),\n",
              " Tool(name='duckduck', description='A web search engine. Use this to as a search engine for general queries.', func=<bound method BaseTool.run of DuckDuckGoSearchResults(api_wrapper=DuckDuckGoSearchAPIWrapper(region='wt-wt', safesearch='moderate', time='y', max_results=5, backend='auto', source='text'))>)]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import AgentExecutor, create_react_agent\n",
        "\n",
        "#Step1 - create agent\n",
        "agent = create_react_agent(open_ai_llm, tools=tools, prompt=prompt)\n",
        "agent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fev_TDwts3m3",
        "outputId": "5df57b01-1c69-4da2-8eed-404e686afbf4"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RunnableAssign(mapper={\n",
              "  agent_scratchpad: RunnableLambda(lambda x: format_log_to_str(x['intermediate_steps']))\n",
              "})\n",
              "| PromptTemplate(input_variables=['agent_scratchpad', 'input'], input_types={}, partial_variables={'tools': \"Calculator(*args: Any, callbacks: Union[list[langchain_core.callbacks.base.BaseCallbackHandler], langchain_core.callbacks.base.BaseCallbackManager, NoneType] = None, tags: Optional[list[str]] = None, metadata: Optional[dict[str, Any]] = None, **kwargs: Any) -> Any - Useful for when you need to answer questions about math.\\nduckduck(tool_input: 'Union[str, dict[str, Any]]', verbose: 'Optional[bool]' = None, start_color: 'Optional[str]' = 'green', color: 'Optional[str]' = 'green', callbacks: 'Callbacks' = None, *, tags: 'Optional[list[str]]' = None, metadata: 'Optional[dict[str, Any]]' = None, run_name: 'Optional[str]' = None, run_id: 'Optional[uuid.UUID]' = None, config: 'Optional[RunnableConfig]' = None, tool_call_id: 'Optional[str]' = None, **kwargs: 'Any') -> 'Any' - A web search engine. Use this to as a search engine for general queries.\", 'tool_names': 'Calculator, duckduck'}, template='Answer the following questions as best you can. You have access to the following tools:\\n\\n{tools}\\n\\nUse the following format :\\n\\nQuestion : the input question you must answer\\nThought : you should always think about what to do\\nAction: The action to take, should be one of [{tool_names}]\\nAction Input : The input to the action\\nObservation : The result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought : I now know the final answer\\nFinal Answer : The final answer to the original input question\\n\\nBegin\\n\\nQuestion : {input}\\nThought : {agent_scratchpad}\\n\\n')\n",
              "| RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7809bdbeed50>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7809bd645e90>, root_client=<openai.OpenAI object at 0x7809c06de750>, root_async_client=<openai.AsyncOpenAI object at 0x7809bd645b50>, temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True), kwargs={'stop': ['\\nObservation']}, config={}, config_factories=[])\n",
              "| ReActSingleInputOutputParser()"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#step 2  - give agent executor the agent object\n",
        "agent_exec = AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True)\n",
        "agent_exec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4H6cCeWtfKW",
        "outputId": "18ee47ff-4440-4887-fd95-1b74ef585268"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AgentExecutor(verbose=True, agent=RunnableAgent(runnable=RunnableAssign(mapper={\n",
              "  agent_scratchpad: RunnableLambda(lambda x: format_log_to_str(x['intermediate_steps']))\n",
              "})\n",
              "| PromptTemplate(input_variables=['agent_scratchpad', 'input'], input_types={}, partial_variables={'tools': \"Calculator(*args: Any, callbacks: Union[list[langchain_core.callbacks.base.BaseCallbackHandler], langchain_core.callbacks.base.BaseCallbackManager, NoneType] = None, tags: Optional[list[str]] = None, metadata: Optional[dict[str, Any]] = None, **kwargs: Any) -> Any - Useful for when you need to answer questions about math.\\nduckduck(tool_input: 'Union[str, dict[str, Any]]', verbose: 'Optional[bool]' = None, start_color: 'Optional[str]' = 'green', color: 'Optional[str]' = 'green', callbacks: 'Callbacks' = None, *, tags: 'Optional[list[str]]' = None, metadata: 'Optional[dict[str, Any]]' = None, run_name: 'Optional[str]' = None, run_id: 'Optional[uuid.UUID]' = None, config: 'Optional[RunnableConfig]' = None, tool_call_id: 'Optional[str]' = None, **kwargs: 'Any') -> 'Any' - A web search engine. Use this to as a search engine for general queries.\", 'tool_names': 'Calculator, duckduck'}, template='Answer the following questions as best you can. You have access to the following tools:\\n\\n{tools}\\n\\nUse the following format :\\n\\nQuestion : the input question you must answer\\nThought : you should always think about what to do\\nAction: The action to take, should be one of [{tool_names}]\\nAction Input : The input to the action\\nObservation : The result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought : I now know the final answer\\nFinal Answer : The final answer to the original input question\\n\\nBegin\\n\\nQuestion : {input}\\nThought : {agent_scratchpad}\\n\\n')\n",
              "| RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7809bdbeed50>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7809bd645e90>, root_client=<openai.OpenAI object at 0x7809c06de750>, root_async_client=<openai.AsyncOpenAI object at 0x7809bd645b50>, temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True), kwargs={'stop': ['\\nObservation']}, config={}, config_factories=[])\n",
              "| ReActSingleInputOutputParser(), input_keys_arg=[], return_keys_arg=[], stream_runnable=True), tools=[Tool(name='Calculator', description='Useful for when you need to answer questions about math.', func=<bound method Chain.run of LLMMathChain(verbose=False, llm_chain=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\"37593 * 67\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\n```text\\n37593**(1/5)\\n```\\n...numexpr.evaluate(\"37593**(1/5)\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nQuestion: {question}\\n'), llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7809bdbeed50>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7809bd645e90>, root_client=<openai.OpenAI object at 0x7809c06de750>, root_async_client=<openai.AsyncOpenAI object at 0x7809bd645b50>, temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True), output_parser=StrOutputParser(), llm_kwargs={}))>, coroutine=<bound method Chain.arun of LLMMathChain(verbose=False, llm_chain=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['question'], input_types={}, partial_variables={}, template='Translate a math problem into a expression that can be executed using Python\\'s numexpr library. Use the output of running this code to answer the question.\\n\\nQuestion: ${{Question with math problem.}}\\n```text\\n${{single line mathematical expression that solves the problem}}\\n```\\n...numexpr.evaluate(text)...\\n```output\\n${{Output of running the code}}\\n```\\nAnswer: ${{Answer}}\\n\\nBegin.\\n\\nQuestion: What is 37593 * 67?\\n```text\\n37593 * 67\\n```\\n...numexpr.evaluate(\"37593 * 67\")...\\n```output\\n2518731\\n```\\nAnswer: 2518731\\n\\nQuestion: 37593^(1/5)\\n```text\\n37593**(1/5)\\n```\\n...numexpr.evaluate(\"37593**(1/5)\")...\\n```output\\n8.222831614237718\\n```\\nAnswer: 8.222831614237718\\n\\nQuestion: {question}\\n'), llm=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x7809bdbeed50>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7809bd645e90>, root_client=<openai.OpenAI object at 0x7809c06de750>, root_async_client=<openai.AsyncOpenAI object at 0x7809bd645b50>, temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True), output_parser=StrOutputParser(), llm_kwargs={}))>), Tool(name='duckduck', description='A web search engine. Use this to as a search engine for general queries.', func=<bound method BaseTool.run of DuckDuckGoSearchResults(api_wrapper=DuckDuckGoSearchAPIWrapper(region='wt-wt', safesearch='moderate', time='y', max_results=5, backend='auto', source='text'))>)], handle_parsing_errors=True)"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent_exec.invoke({\n",
        "    \"input\" : \"What is the current price of a MacBook Pro in USD? How much would it cost in EUR if the exchange rate is 0.85 EUR for 1 USD?\" #input is part of the prompt template\n",
        "})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLL9HsVytzEn",
        "outputId": "7fd6022d-b2bd-41c3-f126-489c7e269017"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3mThought : I should use a web search engine to find the current price of a MacBook Pro in USD and then use a calculator to convert it to EUR.\n",
            "Action: duckduck\n",
            "Action Input: \"current price of MacBook Pro in USD\"\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:primp.impersonate:Impersonate 'chrome_107' does not exist, using 'random'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33;1m\u001b[1;3msnippet: Get deals and low prices on Apple Macbook Pro 16 M4 Max at Amazon. Play games & watch movies with a wide selection of computers & tablets. Types : Automotive, Books, Fashion, Gaming, Gift Cards, Groceries and more, title: Apple Macbook Pro 16 M4 Max - See Our New Releases, link: https://www.bing.com/aclick?ld=e8IKXuFgsJajlh8y6y7Fj2bTVUCUwbl6IywkfRvYt_pf0w0AtToK_qw_Ea9mtC-sgSkPdjD8mC83DqBUyQ7CTbRRnZ61ZJ3faiSch-1w9H-X5KguG55nw1GY-CbJBBfCjGFM3Fi39qnw3sgssWe66kP_QCNFAV0UYtVl23mZ0yY39Hbgbt2smvjhXYlWwufiGp6lISKA&u=aHR0cHMlM2ElMmYlMmZ3d3cuYW1hem9uLmNvbSUyZnMlMmYlM2ZpZSUzZFVURjglMjZrZXl3b3JkcyUzZGFwcGxlJTJibWFjYm9vayUyYnBybyUyYjE2JTJibTQlMmJtYXglMjZpbmRleCUzZGFwcyUyNnRhZyUzZG1oMGItMjAlMjZyZWYlM2RwZF9zbF82a2RwbmFmdHBhX2IlMjZhZGdycGlkJTNkMTMzOTIwNzY5NzA0NjI5NiUyNmh2YWRpZCUzZDgzNzAwNzU5MTA5MzAzJTI2aHZuZXR3JTNkbyUyNmh2cW10JTNkYiUyNmh2Ym10JTNkYmIlMjZodmRldiUzZGMlMjZodmxvY2ludCUzZCUyNmh2bG9jcGh5JTNkMTUxNTcwJTI2aHZ0YXJnaWQlM2Rrd2QtODM3MDE2NjUyNjgyOTklM2Fsb2MtMTI5JTI2aHlkYWRjciUzZDI1Njg0XzEzNDY0NzkzJTI2bWNpZCUzZDI1NTJjMDFmNjc4OTM3Mzk5OGQ1OWYxZDI2MjAyODA5JTI2bXNjbGtpZCUzZGE3MTA4ZDhiNjIzNjEwNTUwYWY5ZTQxOGMxZWNhMWQ5&rlid=a7108d8b623610550af9e418c1eca1d9, snippet: Buy MacBook Pro From $1599 or $133.25/mo. for 12 mo. ‡ Add a trade-in to save on your new Mac. ∆ Get 3% Daily Cash back with Apple Card., title: Buy MacBook Pro - Apple, link: https://www.apple.com/shop/buy-mac/macbook-pro, snippet: Feb 17, 2026 · Shop Best Buy for the MacBook Pro . Take your work to the next level with Apple’s MacBook Pro with Retina display and create stunning graphics with ease., title: Apple MacBook Pro - Best Buy, link: https://www.bestbuy.com/site/macbooks/macbook-pro/pcmcat378600050009.c?id=pcmcat378600050009, snippet: Nov 12, 2025 · List prices of MacBook Pro 14-inch M5 16GB 512GB in United States. Find the best deals and compare with other countries in USD ., title: MacBook Pro in United States | USD Prices – The Mac Index, link: https://themacindex.com/us/products/macbook-pro\u001b[0m\u001b[32;1m\u001b[1;3mAction: duckduck\n",
            "Action Input: \"current price of MacBook Pro in USD\"\u001b[0m\u001b[33;1m\u001b[1;3msnippet: Shop Best Buy for the MacBook Pro . Take your work to the next level with Apple's MacBook Pro with Retina display and create stunning graphics with ease., title: Apple MacBook Pro - Best Buy, link: https://www.bestbuy.com/site/macbooks/macbook-pro/pcmcat378600050009.c?id=pcmcat378600050009, snippet: Find the cheapest prices and best deals on current MacBook Pros at AppleInsider., title: Best MacBook Pro Deals 2026 | Save up to $514 on M5, M4 Pro, link: https://appleinsider.com/deals/best-macbook-pro-deals, snippet: Amazon today has dropped the price of the new M5 MacBook Pro to $1,349.99, down from $1,599.00. This is the 10-Core model with 16GB RAM and 512GB SSD, and it's a match of the all-time low price on ..., title: Apple's New M5 MacBook Pro Hits $1,349.99 on Amazon ($249 Off), link: https://www.macrumors.com/2025/12/18/m5-macbook-pro-to-1349/, snippet: Deals 9to5Toys 9to5Toys Lunch Break Deals: M5 MacBook Pro $300 off, 13-inch MacBook Air $250 off, AirTag 2 lowest price yet, more Justin Kahn | Feb 13 2026 - 8:44 am PT 0 Comments, title: Deals: M5 MacBook Pro, MacBook Air, AirTag 2, more 9to5Mac, link: https://9to5mac.com/2026/02/13/deals-m5-macbook-pro-macbook-air-airtag-2/\u001b[0m\u001b[32;1m\u001b[1;3mAction: duckduck\n",
            "Action Input: \"current price of MacBook Pro in USD\"\u001b[0m\u001b[33;1m\u001b[1;3msnippet: MacBook Pro laptop with M5, M4 Pro , and M4 Max chips. Built for AI and Apple Intelligence.The most advanced Mac laptops for demanding workflows. * Currently Viewing, MacBook Pro . Buy., title: MacBook Pro - Apple, link: https://www.apple.com/macbook-pro/, snippet: Apple 2020 MacBook Pro with 2.0GHz Intel Core i5, 13-inch, 16GB RAM, 512GB SSD Storage - Space Gray (Renewed)., title: Amazon.com: Macbook Pro, link: https://www.amazon.com/macbook-pro/s?k=macbook+pro, snippet: This choice will most directly factor into how much you pay for the notebook, with 16-inch options typically starting at and exceeding the $2,500 price point, and 14-inch options coming in below that. Apple updated the 14-inch MacBook Pro base model with the M5 chip in October 2025., title: Best MacBook Pro and MacBook Air Deals for February... - MacRumors, link: https://www.macrumors.com/guide/macbook-pro-air-deals/, snippet: BestReviews.guide analyzes thousands of articles and customer reviews to find the top-rated products at today's lowest prices .MARGOLAI 14 inch Laptop Computer, 8GB DDR4 RAM 128GB SSD, 4-Core Processor (Up to 2.9GHz), Thin & Portable Notebook PC, Win11 Pro , WiFi, BT4.2., title: 10 Best Price Of Macbooks 2026 in the US | BestReviews.guide, link: https://www.bestreviews.guide/price-of-macbooks\u001b[0m\u001b[32;1m\u001b[1;3mAction: duckduck\n",
            "Action Input: \"current price of MacBook Pro in USD\"\u001b[0m\u001b[33;1m\u001b[1;3msnippet: MacBook Pro laptop with M5, M4 Pro , and M4 Max chips. Built for AI and Apple Intelligence.The most advanced Mac laptops for demanding workflows. * Currently Viewing, MacBook Pro . Buy., title: MacBook Pro - Apple, link: https://www.apple.com/macbook-pro/, snippet: Apple 2020 MacBook Pro with 2.0GHz Intel Core i5, 13-inch, 16GB RAM, 512GB SSD Storage - Space Gray (Renewed)., title: Amazon.com: Macbook Pro, link: https://www.amazon.com/macbook-pro/s?k=macbook+pro, snippet: This choice will most directly factor into how much you pay for the notebook, with 16-inch options typically starting at and exceeding the $2,500 price point, and 14-inch options coming in below that. Apple updated the 14-inch MacBook Pro base model with the M5 chip in October 2025., title: Best MacBook Pro and MacBook Air Deals for February... - MacRumors, link: https://www.macrumors.com/guide/macbook-pro-air-deals/, snippet: BestReviews.guide analyzes thousands of articles and customer reviews to find the top-rated products at today's lowest prices .MARGOLAI 14 inch Laptop Computer, 8GB DDR4 RAM 128GB SSD, 4-Core Processor (Up to 2.9GHz), Thin & Portable Notebook PC, Win11 Pro , WiFi, BT4.2., title: 10 Best Price Of Macbooks 2026 in the US | BestReviews.guide, link: https://www.bestreviews.guide/price-of-macbooks\u001b[0m\u001b[32;1m\u001b[1;3mAction: Calculator\n",
            "Action Input: 1349.99 * 0.85\u001b[0m\u001b[36;1m\u001b[1;3mAnswer: 1147.4915\u001b[0m\u001b[32;1m\u001b[1;3mFinal Answer: The current price of a MacBook Pro in USD is $1349.99. If the exchange rate is 0.85 EUR for 1 USD, it would cost approximately 1147.49 EUR.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What is the current price of a MacBook Pro in USD? How much would it cost in EUR if the exchange rate is 0.85 EUR for 1 USD?',\n",
              " 'output': 'The current price of a MacBook Pro in USD is $1349.99. If the exchange rate is 0.85 EUR for 1 USD, it would cost approximately 1147.49 EUR.'}"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm",
      "runtime_attributes": {
        "runtime_version": "2025.07"
      }
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}